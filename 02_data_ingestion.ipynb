{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to follow \n",
    "**Step 1 :** download the Google cloud cli \n",
    "\n",
    "**Step 2 :** Install the google cloud SDK using the following command:\n",
    "\n",
    "```bash\n",
    "curl https://sdk.cloud.google.com | bash\n",
    "```\n",
    "\n",
    "**Step 3 :** Configure the Google Cloud SDK\n",
    "```bash\n",
    "gcloud init\n",
    "```\n",
    "\n",
    "**Step 4 :** Authenticate your Google Cloud SDK with your Google Cloud account\n",
    "```bash\n",
    "gcloud auth login\n",
    "```\n",
    "**Step 5 :** Set the project ID for your Google Cloud project\n",
    "```bash\n",
    "gcloud config set project <project_id>\n",
    "```\n",
    "**Step 6 :** Create a new Google Cloud Storage bucket\n",
    "```bash\n",
    "gsutil mb -p <project_id> gs://<bucket_name>\n",
    "```\n",
    "**Step 7 :** Upload your local data to the Google Cloud Storage bucket\n",
    "```bash\n",
    "gsutil cp -r <local_data_dir> gs://<bucket_name>\n",
    "```\n",
    "**Step 8 :** Create a new Google Cloud Machine Learning Engine (ML Engine) cluster\n",
    "```bash\n",
    "gcloud ml-engine clusters create <cluster_name> --num-nodes 3\n",
    "```\n",
    "**Step 9 :** Set the Google Cloud ML Engine region\n",
    "```bash\n",
    "gcloud config set ml.googleapis.com/region <region>\n",
    "```\n",
    "**Step 10 :** Build and push the Docker image\n",
    "```bash\n",
    "docker build -t gcr.io/<project_id>/<cluster_name>:latest.\n",
    "```\n",
    "**Step 11 :** Deploy the trained model to Google Cloud ML Engine\n",
    "```bash\n",
    "gcloud ml-engine models create <model_name> --regions <region>\n",
    "```\n",
    "```bash\n",
    "gcloud ml-engine versions create v1 --model <model_name> --runtime-version 1.15 --image gcr.io/<project_id>/<cluster_name>:latest\n",
    "```\n",
    "**Step 12 :** Create a new Google Cloud Run service\n",
    "```bash\n",
    "gcloud run deploy --image gcr.io/<project_id>/<cluster_name>:latest --platform managed --region <region> --allow-unauthenticated --update-env-vars PROJECT_NAME=<project_name>\n",
    "```\n",
    "**Step 13 :** Test the deployed model\n",
    "```bash\n",
    "gcloud run services describe <service_name> --platform managed --region <region> --format=json | jq -r '.status.url'\n",
    "```\n",
    "**Step 14 :** Delete the Google Cloud resources\n",
    "```bash\n",
    "gcloud run services delete <service_name> --platform managed --region <region> --quiet\n",
    "```\n",
    "**Step 15 :** Delete the Google Cloud Storage bucket\n",
    "```bash\n",
    "gsutil rm -r gs://<bucket_name>\n",
    "```\n",
    "**Step 16 :** Delete the Google Cloud ML Engine cluster\n",
    "```bash\n",
    "gcloud ml-engine clusters delete <cluster_name> --quiet\n",
    "```\n",
    "**Step 17 :** Delete the Google Cloud Run service\n",
    "```bash\n",
    "gcloud run services delete <service_name> --platform managed --region <region> --quiet\n",
    "```\n",
    "**Step 18 :** Delete the Google Cloud project\n",
    "```bash\n",
    "gcloud projects delete <project_id> --quiet\n",
    "```\n",
    "**Note: This is a basic guideline and actual steps may vary based on your project requirements and the specific Google Cloud services you are using. Make sure to replace placeholders with your actual project details and configurations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up Gcloud cli for project and  bucket for google cloud services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name=\"Hotel-Reservation-Prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 :** download the Google cloud cli \n",
    "\n",
    "**Step 2 :** Install the google cloud SDK using the following command:\n",
    "\n",
    "```bash\n",
    "curl https://sdk.cloud.google.com | bash\n",
    "```\n",
    "\n",
    "**Step 3 :** Configure the Google Cloud SDK\n",
    "```bash\n",
    "gcloud init\n",
    "```\n",
    "\n",
    "**Step 4 :** Authenticate your Google Cloud SDK with your Google Cloud account\n",
    "```bash\n",
    "gcloud auth login\n",
    "```\n",
    "**Step 5 :** Set the project ID for your Google Cloud project\n",
    "```bash\n",
    "gcloud config set project <project_id>\n",
    "```\n",
    "**Step 6 :** Create a new Google Cloud Storage bucket\n",
    "```bash\n",
    "gsutil mb -p <project_id> gs://<bucket_name>\n",
    "```\n",
    "**Step 7 :** Upload your local data to the Google Cloud Storage bucket\n",
    "```bash\n",
    "gsutil cp -r <local_data_dir> gs://<bucket_name>\n",
    "```\n",
    "step 8: create service account\n",
    "\n",
    "To create a service account, follow these steps:\n",
    "\n",
    "1. Go to the Google Cloud Console.\n",
    "2. Go to IAM & Admin.\n",
    "3. Click on \"Create service account\".\n",
    "4. Give the service account a name and select the role **\"Storage Admin\"** and  **\"Storage Object Viewer\"**.\n",
    "5. Go to the bucket you want to grant access to. And click on 3dots to select **Edit Access**  \n",
    "6. Under on **Add Principal** add **your service account** and Assign Roles **\"Storage Admin\"** and  **\"Storage Object Viewer\"** and click on **Save** \n",
    "7. Goto your service account created befor in Iam & Admin Section, Click on **3dots:**  and **add key** to download key in JSON key file for the service account.\n",
    "\n",
    "``` bash \n",
    "set GOOGLE_APPLICATION_CREDENTIALS=\"C:\\\\path to the Json file downloaded before using above step\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Mlops_Udemy\\\\Hotel Reservation Prediction'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Mlops_Udemy\\\\Hotel Reservation Prediction'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a config/config.yaml\n",
    "\n",
    "data_ingestion:\n",
    "  bucket_name : \"my_bucket9789\"\n",
    "  bucket_file_name : \"Hotel_Reservations.csv\"\n",
    "  train_ratio : 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to config/path_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a config/path_config.py\n",
    "\n",
    "import os\n",
    "\n",
    "########################### DATA INGESTION #########################\n",
    "\n",
    "RAW_DIR = \"artifacts/raw\"\n",
    "RAW_FILE_PATH = os.path.join(RAW_DIR,\"raw.csv\")\n",
    "TRAIN_FILE_PATH = os.path.join(RAW_DIR,\"train.csv\")\n",
    "TEST_FILE_PATH = os.path.join(RAW_DIR,\"test.csv\")\n",
    "\n",
    "CONFIG_PATH = \"config/config.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/data_ingestion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/data_ingestion.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.logger import get_logger\n",
    "from src.custom_exception import CustomException\n",
    "from config.paths_config import *\n",
    "from utils.common_functions import read_yaml\n",
    "\n",
    "logger = get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/data_ingestion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/data_ingestion.py\n",
    "class DataIngestion:\n",
    "    def __init__(self,config):\n",
    "        self.config = config[\"data_ingestion\"]\n",
    "        self.bucket_name = self.config[\"bucket_name\"]\n",
    "        self.file_name = self.config[\"bucket_file_name\"]\n",
    "        self.train_test_ratio = self.config[\"train_ratio\"]\n",
    "\n",
    "        os.makedirs(RAW_DIR , exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Data Ingestion started with {self.bucket_name} and file is  {self.file_name}\")\n",
    "\n",
    "    def download_csv_from_gcp(self):\n",
    "        try:\n",
    "            client = storage.Client()\n",
    "            bucket = client.bucket(self.bucket_name)\n",
    "            blob = bucket.blob(self.file_name)\n",
    "\n",
    "            blob.download_to_filename(RAW_FILE_PATH)\n",
    "\n",
    "            logger.info(f\"CSV file is sucesfully downloaded to {RAW_FILE_PATH}\")\n",
    "\n",
    "        except Exception as e: \n",
    "            logger.error(\"Error while downloading the csv file\")\n",
    "            raise CustomException(\"Failed to downlaod csv file \", e)\n",
    "        \n",
    "    def split_data(self):\n",
    "        try:\n",
    "            logger.info(\"Starting the splitting process\")\n",
    "            data = pd.read_csv(RAW_FILE_PATH)\n",
    "            train_data , test_data = train_test_split(data , test_size=1-self.train_test_ratio , random_state=42)\n",
    "\n",
    "            train_data.to_csv(TRAIN_FILE_PATH)\n",
    "            test_data.to_csv(TEST_FILE_PATH)\n",
    "\n",
    "            logger.info(f\"Train data saved to {TRAIN_FILE_PATH}\")\n",
    "            logger.info(f\"Test data saved to {TEST_FILE_PATH}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(\"Error while splitting data\")\n",
    "            raise CustomException(\"Failed to split data into training and test sets \", e)\n",
    "        \n",
    "    def run(self):\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Starting data ingestion process\")\n",
    "\n",
    "            self.download_csv_from_gcp()\n",
    "            self.split_data()\n",
    "\n",
    "            logger.info(\"Data ingestion completed sucesfully\")\n",
    "        \n",
    "        except CustomException as ce:\n",
    "            logger.error(f\"CustomException : {str(ce)}\")\n",
    "        \n",
    "        finally:\n",
    "            logger.info(\"Data ingestion completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_ingestion = DataIngestion(read_yaml(CONFIG_PATH))\n",
    "    data_ingestion.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pipeline/traing_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pipeline/traing_pipeline.py\n",
    "from utils.common_functions import read_yaml\n",
    "from config.paths_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pipeline/traing_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pipeline/traing_pipeline.py\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    ### 1. Data Ingestion\n",
    "    from src.data_ingestion import DataIngestion\n",
    "    data_ingestion = DataIngestion(read_yaml(CONFIG_PATH))\n",
    "    data_ingestion.run()\n",
    "    ############################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hotel_reserv_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
